{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd06038c0ee8db233780d2258e0f517839f1e0d675968383c2c072a296dd1a5eb9c",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Load the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "< ---- Basic information ---- >\n",
      "- 9851 base stations. 4171950 entries\n",
      "\n",
      "< ---- Unique users ---- >\n",
      "- 871 unique users\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "# pd.options.display.max_columns\n",
    "pd.set_option(\"display.max_colwidth\",200)\n",
    "pd.set_option(\"display.max_columns\",20)\n",
    "pd.set_option('float_format', '{:.3f}'.format)\n",
    "\n",
    "# Load Usage data\n",
    "with open('App_usage_trace.txt') as f:\n",
    "    lines = f.readlines()#[:100000]\n",
    "    df = []\n",
    "    for line in lines:\n",
    "        data = line.split()\n",
    "        df.append(data)\n",
    "usage = pd.DataFrame(df, columns=['uid','timestamp', 'loc', 'app_id','traffic'])\n",
    "# output_user(usage)\n",
    "usage['traffic'] = usage['traffic'].astype('float64') / 1e6 # Convert traffic to MB\n",
    "usage['timestamp'] = usage['timestamp'].apply(lambda x: datetime.strptime(x, \"%Y%m%d%H%M%S\")) # Convert to datetime object\n",
    "usage['loc'] = usage['loc'].astype('int64')\n",
    "\n",
    "# Load App2Category data\n",
    "with open('App2Category.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    df = []\n",
    "    for line in lines:\n",
    "        data = line.split()\n",
    "        df.append(data)\n",
    "app2cat = pd.DataFrame(df, columns=['app_id','cat_id'])\n",
    "\n",
    "# Load base station POI data\n",
    "base_poi = pd.read_csv(\"base_poi.txt\", delimiter='\\t')\n",
    "\n",
    "# Load Category dictionary\n",
    "cat = pd.read_csv(\"Categorys.txt\", delimiter='\\t', header=None)\n",
    "cat.columns = ['cat_id','category']\n",
    "cat.set_index('cat_id', inplace=True)\n",
    "\n",
    "\n",
    "print(\"< ---- Basic information ---- >\")\n",
    "print(\"- {} base stations. {} entries\".format(base_poi.shape[0], usage.shape[0]))\n",
    "print()\n",
    "print(\"< ---- Unique users ---- >\")\n",
    "print(\"- {} unique users\".format(len(usage['uid'].unique())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 601 µs\n"
     ]
    }
   ],
   "source": [
    "def output_user(data):\n",
    "    # group data into users\n",
    "    grouped_users = data.groupby('uid')\n",
    "    for user, group in grouped_users:\n",
    "        # iterare each user and output to seperate file\n",
    "        group.to_csv('user_{}.txt'.format(user), header=False, index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 46.9 ms\n"
     ]
    }
   ],
   "source": [
    "# Load base station POI data\n",
    "base_poi = pd.read_csv(\"base_poi.txt\", delimiter='\\t')\n",
    "\n",
    "base_poi.set_index('BaseID', inplace=True)\n",
    "base_poi['total'] = base_poi.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 762 ms\n"
     ]
    }
   ],
   "source": [
    "# Group user count to find heavy users uid later\n",
    "user_count = usage.groupby([\"uid\"]).size().reset_index()\n",
    "user_count.columns = ['uid','count']"
   ]
  },
  {
   "source": [
    "## Just to use some simple rules to remove users / base stations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 2.5 ms\n"
     ]
    }
   ],
   "source": [
    "def remove_records(n_user=2000, n_base=5):\n",
    "    # Remove user with less than n records\n",
    "    user_count_clean = user_count[user_count['count'] > n_user]\n",
    "    print('='*50)\n",
    "    print(\"We remove users with less than {} records\".format(n_user))\n",
    "    print(\"-\"*50)\n",
    "    print(\"# of base station before cleaning:\", user_count.shape[0])\n",
    "    print(\"# of users after cleaning:\", user_count_clean.shape[0])\n",
    "    print()\n",
    "    # Store the uid that we removed\n",
    "    removed_user = user_count[user_count['count'] < n_user].index\n",
    "    removed_user = list(removed_user)\n",
    "    print(\"Removed {} users and returned a list of removed uid\".format(len(removed_user)))\n",
    "    print()\n",
    "    print('='*50)\n",
    "    # Remove Base station with less than n POI\n",
    "    print(\"We remove base station with less than {} POI\".format(n_base))\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    print(\"# of base station before cleaning:\", base_poi.shape[0])\n",
    "    base_poi_clean = base_poi[base_poi['total'] > n_base]\n",
    "    print(\"# of base station after cleaning:\", base_poi_clean.shape[0])\n",
    "    print()\n",
    "    # Store the baseID we are removing\n",
    "    removed_base = base_poi[base_poi['total'] < n_base].index\n",
    "    removed_base = list(removed_base)\n",
    "    print(\"Removed {} base and returned a list of removed baseID\".format(len(removed_base)))\n",
    "\n",
    "    # Remove the records that match either the removed user or removed baseID\n",
    "    # Return the dataset\n",
    "\n",
    "    return removed_user, removed_base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================================================\nWe remove users with less than 1000 records\n--------------------------------------------------\n# of base station before cleaning: 871\n# of users after cleaning: 560\n\nRemoved 311 users and returned a list of removed uid\n\n==================================================\nWe remove base station with less than 20 POI\n--------------------------------------------------\n# of base station before cleaning: 9851\n# of base station after cleaning: 5768\n\nRemoved 3947 base and returned a list of removed baseID\ntime: 17.7 ms\n"
     ]
    }
   ],
   "source": [
    "removed_user, removed_base = remove_records(1000, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 633 µs\n"
     ]
    }
   ],
   "source": [
    "def output_user(data):\n",
    "    grouped_users = data.groupby('uid')\n",
    "    for user, group in grouped_users:\n",
    "        group.to_csv('user_{}.txt'.format(user), header=False, index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Nathans Approach"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 3.64 ms\n"
     ]
    }
   ],
   "source": [
    "# # Function to align the Origin and Destination (O/D)\n",
    "# def sort_movement_tuples(jump):\n",
    "#     return sorted(list(jump))\n",
    "\n",
    "# # Define a function that can output the frequency pair of a user's mobility pattern\n",
    "# def show_frequent_pair(uid=888, threshold=5, jump_occurence=10):\n",
    "#     # Nathan: replaced usage with usage_cleaned\n",
    "#     user_uid = usage[usage['uid'] == str(uid)]\n",
    "\n",
    "#     # Set the thresold of time gap allowed between movement\n",
    "#     delta_threshold = timedelta(seconds=threshold)\n",
    "\n",
    "#     # Create the columns of the next time and lcoation\n",
    "#     user_uid['next_loc'] = user_uid['loc'].shift(-1).fillna(0).astype('int')\n",
    "#     user_uid['next_timestamp'] = user_uid['timestamp'].shift(-1)\n",
    "\n",
    "#     # Keep only movements\n",
    "#     user_uid = user_uid[user_uid['loc'] != user_uid['next_loc']]\n",
    "\n",
    "#     # Get the time gaps between movements\n",
    "#     user_uid['time_delta'] = user_uid['next_timestamp'] - user_uid['timestamp']\n",
    "\n",
    "#     # Get teleports and count the teleports\n",
    "#     # Should be able to identify the return trips\n",
    "#     teleports = user_uid[user_uid['time_delta'] < delta_threshold]\n",
    "#     teleports['tele'] = list(zip(teleports['loc'], teleports['next_loc']))\n",
    "#     # print(teleports['tele'].value_counts())\n",
    "\n",
    "#     # # Group and count the same jump\n",
    "#     tele_counts = teleports['tele'].value_counts().reset_index()\n",
    "#     tele_counts.columns = ['jump', 'count']\n",
    "\n",
    "#     # Align the O/D of the jump to identify the pairs\n",
    "#     tele_counts['jump'] = tele_counts['jump'].apply(lambda i: sort_movement_tuples(i))\n",
    "\n",
    "#     # Get the jumps that has a reasonable number of occurence\n",
    "#     tele_counts = tele_counts[tele_counts['count'] > jump_occurence]\n",
    "#     tele_counts['jump'] = tuple(tele_counts['jump'])\n",
    "\n",
    "#     # print the record for reference\n",
    "#     # Nathan: added reset_index()\n",
    "#     frequent_pairs = tele_counts.groupby(['jump']).sum().reset_index()\n",
    "\n",
    "#     return frequent_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 1.44 ms\n"
     ]
    }
   ],
   "source": [
    "# Function to align the Origin and Destination (O/D)\n",
    "def sort_movement_tuples(jump):\n",
    "    return sorted(list(jump))\n",
    "\n",
    "# show the period where the oscillation happens\n",
    "def present_oscillation(uid=888, threshold=5, jump_occurence=10):\n",
    "    # Nathan: replaced usage with usage_cleaned\n",
    "    user_uid = usage[usage['uid'] == str(uid)]\n",
    "\n",
    "    # Set the thresold of time gap allowed between movement\n",
    "    delta_threshold = timedelta(seconds=threshold)\n",
    "\n",
    "    # Create the columns of the next time and lcoation\n",
    "    user_uid['next_loc'] = user_uid['loc'].shift(-1).fillna(0).astype('int')\n",
    "    user_uid['next_timestamp'] = user_uid['timestamp'].shift(-1)\n",
    "\n",
    "    # Keep only movements\n",
    "    user_uid = user_uid[user_uid['loc'] != user_uid['next_loc']]\n",
    "\n",
    "    # Get the time gaps between movements\n",
    "    user_uid['time_delta'] = user_uid['next_timestamp'] - user_uid['timestamp']\n",
    "\n",
    "    # Get teleports and count the teleports\n",
    "    # Should be able to identify the return trips\n",
    "    teleports = user_uid[user_uid['time_delta'] < delta_threshold]\n",
    "    teleports['tele'] = list(zip(teleports['loc'], teleports['next_loc']))\n",
    "    # teleports['tele'] = teleports['tele'].apply(lambda i: sort_movement_tuples(i))\n",
    "    # print(teleports['tele'].value_counts())\n",
    "\n",
    "    # # Group and count the same jump\n",
    "    tele_counts = teleports['tele'].value_counts().reset_index()\n",
    "    tele_counts.columns = ['jump', 'count']\n",
    "    tele_counts = tele_counts[tele_counts['count'] > jump_occurence]\n",
    "\n",
    "\n",
    "    # # Align the O/D of the jump to identify the pairs\n",
    "    # tele_counts['jump'] = tele_counts['jump'].apply(lambda i: sort_movement_tuples(i))\n",
    "\n",
    "    # Get the jumps that has a reasonable number of occurence\n",
    "    tele_counts = tele_counts[tele_counts['count'] > jump_occurence]\n",
    "    # tele_counts['jump'] = tuple(tele_counts['jump'])\n",
    "\n",
    "    #return the rows that has tele_count in the \"tele\" column, so it returns the rows where the oscillation happens\n",
    "    return teleports[teleports['tele'].isin(tele_counts[\"jump\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 341 µs\n"
     ]
    }
   ],
   "source": [
    "# # show the oscillation period\n",
    "# user_oscillations = present_oscillation(979,10,5)\n",
    "# # user_oscillations.iloc[:50,:]\n",
    "# # user_oscillations.drop(columns=['next_timestamp','time_delta'])\n",
    "# # Create the columns of the next time and lcoation\n",
    "# user_oscillations['next_timestamp'] = user_oscillations['timestamp'].shift(-1)\n",
    "# user_oscillations['time_delta'] = user_oscillations['next_timestamp'] - user_oscillations['timestamp']\n",
    "# user_oscillations.iloc[:50,:]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 909 µs\n"
     ]
    }
   ],
   "source": [
    "# delta_threshold = timedelta(seconds=300)\n",
    "# count = 0\n",
    "# oscillation_list = []\n",
    "# start = True\n",
    "# for index, request in user_oscillations.iterrows():\n",
    "#     if start:\n",
    "#         start_index = index\n",
    "#         start = False\n",
    "#     # print(request['time_delta'])\n",
    "#     if request['time_delta'] > delta_threshold:\n",
    "#         end_index = index\n",
    "#         # print(index)\n",
    "#         # oscillation_list.append([(start_index, end_index), request['tele']])\n",
    "#         oscillation_list.append((start_index, end_index))\n",
    "#         start = True\n",
    "\n",
    "# oscillation_list[0:21]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 959 µs\n"
     ]
    }
   ],
   "source": [
    "# def replace_base(data, loc_list):\n",
    "#     # set count to only run a small sample\n",
    "#     count = 0\n",
    "#     for oscillation_period, bases in loc_list:\n",
    "#         start = oscillation_period[0]\n",
    "#         end = oscillation_period[1]\n",
    "#         x = data.loc[start:end + 6,'loc']\n",
    "#         # if count == 5:\n",
    "#             # break\n",
    "#         # if count % 5 == 0:\n",
    "#         #     print(\"iteration:\", count)\n",
    "#         #     print(\"Counts:\\n\", x.value_counts())\n",
    "#         #     # print(bases)\n",
    "#         #     print(\"Before:\\n\", x)\n",
    "#         #https://stackoverflow.com/questions/47136436/python-pandas-convert-value-counts-output-to-dataframe\n",
    "#         base_counts = x.value_counts().rename_axis('loc').reset_index(name='counts')\n",
    "#         # https://stackoverflow.com/questions/37841525/correct-way-to-set-value-on-a-slice-in-pandas\n",
    "#         base_1 = base_counts.loc[base_counts['loc'] == bases[0], 'counts'].iloc[0]\n",
    "#         base_2 = base_counts.loc[base_counts['loc'] == bases[1], 'counts'].iloc[0]\n",
    "#         # further work is to get list of base_poi in value_count. find greatest amongst list and change.\n",
    "#         if base_2 > base_1:\n",
    "#             # x.loc[x['loc'] == bases[0], 'loc'] = bases[1]\n",
    "#             data.loc[start:end,'loc'] = bases[1]\n",
    "#             # replace base 1\n",
    "#         else:\n",
    "#             # x.loc[x['loc'] == bases[1],'loc'] = bases[0]\n",
    "#             data.loc[start:end,'loc'] = bases[0]\n",
    "#             # replace base 2\n",
    "#             # also repalce base 2 if equal since base 1 came first\n",
    "#         # if count % 5 == 0:\n",
    "#         #     print(\"After:\\n\", x)\n",
    "#         count += 1\n",
    "\n",
    "\n",
    "#         # # print(base_counts.iloc[[bases[0]]])\n",
    "#         # print(base_counts.index.values)\n",
    "#         # # print(bases[0], bases[1])\n",
    "#         # # https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe\n",
    "#         # print(base_counts.loc[base_counts['loc'] == bases[0], 'counts'].iloc[0])\n",
    "#         # print(list(bases))\n",
    "#         # # print(base_counts.iloc[list(bases)])\n",
    "#         # print(x['loc'].value_counts())\n",
    "\n",
    "\n",
    "#         # # https://moonbooks.org/Articles/How-to-extract-the-value-names-and-counts-from-valuecounts-in-pandas-/\n",
    "#         # # for idx,location in enumerate(base_counts.index.tolist()):\n",
    "#         #     if bases[0]\n",
    "#             # print('Name :', location)\n",
    "#             # print('Counts :', base_counts.iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 2.41 ms\n"
     ]
    }
   ],
   "source": [
    "# working_usage = usage.copy()\n",
    "# replace_base(working_usage, oscillation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 1.08 ms\n"
     ]
    }
   ],
   "source": [
    "from heapq import nlargest\n",
    "\n",
    "def replace_base1(data, loc_list):\n",
    "    for oscillation_period in loc_list:\n",
    "        # get starting index\n",
    "        start = oscillation_period[0]\n",
    "        # get ending index of oscillation period\n",
    "        end = oscillation_period[1]\n",
    "        # slice dataset to get requests within oscilation period\n",
    "        x = data.loc[start:end+1,'loc']\n",
    "        # get the count of each base station. \n",
    "        #   rename axis and reset index to get row index as column\n",
    "        base_counts = x.value_counts().rename_axis('loc').reset_index(name='counts')\n",
    "        # get list of unique base stations in oscilation period\n",
    "        base_stations = base_counts['loc'].tolist()\n",
    "        # get inital count of each base station within the oscillation period\n",
    "        base_station_counts = base_counts['counts'].tolist()\n",
    "        # get difference between the top 2 base stations\n",
    "        cur_diff = nlargest(2, base_station_counts)[0] - nlargest(2, base_station_counts)[1]\n",
    "        # we only declare one base station as majority if it has a count greater than 5 to the next greatest\n",
    "        i =1\n",
    "        while cur_diff < 5:\n",
    "            # we take into account of the next surrounding requests outside of the oscillation period\n",
    "            # this will continue to increase by 1 request at each side\n",
    "            x = data.loc[start - i :end + i,'loc']\n",
    "\n",
    "            # get new count of base stations in oscillation period\n",
    "            base_counts = x.value_counts().rename_axis('loc').reset_index(name='counts')\n",
    "            # output count as list\n",
    "            base_station_counts = base_counts['counts'].tolist()\n",
    "            # get the difference\n",
    "            cur_diff = nlargest(2, base_station_counts)[0] - nlargest(2, base_station_counts)[1]\n",
    "            i += 1\n",
    "\n",
    "        # assign variable with the majority base station in oscillation period\n",
    "        largest_station = base_stations[base_station_counts.index(max(base_station_counts))]\n",
    "\n",
    "        # manipulate original dataframe to make changes perm \n",
    "        data.loc[start:end+1,'loc'] = largest_station\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 823 µs\n"
     ]
    }
   ],
   "source": [
    "# working_usage = usage.copy()\n",
    "# replace_base1(working_usage, oscillation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 8min 57s\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# get list of all unqiue users in dataset\n",
    "user_list = usage['uid'].unique()\n",
    "# copy orignal dataset\n",
    "# this is for comparison purposes\n",
    "working_usage = usage.copy()\n",
    "for uid in user_list: \n",
    "    # get all oscillations from current user\n",
    "    # set threshold as 10 seconds and atleast 5 oscillation pairs\n",
    "    user_oscillations = present_oscillation(uid,10,5)\n",
    "\n",
    "    # Create the columns of the next time and lcoation between frequent pairs\n",
    "    user_oscillations['next_timestamp'] = user_oscillations['timestamp'].shift(-1)\n",
    "    user_oscillations['time_delta'] = user_oscillations['next_timestamp'] - user_oscillations['timestamp']\n",
    "\n",
    "    # set delta threshold between oscillation periods\n",
    "    delta_threshold = timedelta(seconds=300)\n",
    "    # declare list to hold indicies of oscillation periods\n",
    "    oscillation_list = []\n",
    "\n",
    "    start = True\n",
    "    for index, request in user_oscillations.iterrows():\n",
    "        # set the first index\n",
    "        if start:\n",
    "            start_index = index\n",
    "            start = False\n",
    "\n",
    "        # if delta to next oscillation request is greater than threshold then indicate this is the last index of current period\n",
    "        if request['time_delta'] > delta_threshold:\n",
    "            # set end index for current period\n",
    "            end_index = index\n",
    "            # append tuple (starting index, ending index) of the oscillation period\n",
    "            oscillation_list.append((start_index, end_index))\n",
    "            # declare the next request will be the start of the next oscillation period\n",
    "            start = True     \n",
    "    # replace all osillations for current user to dataframe        \n",
    "    replace_base1(working_usage, oscillation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 263 µs\n"
     ]
    }
   ],
   "source": [
    "# count = 0\n",
    "# for indexes, bases in oscillation_list:\n",
    "#     if count % 50 == 0:\n",
    "#         print(count)\n",
    "#         print(working_usage.iloc[indexes[0]:indexes[1]+6,:]['loc'].value_counts())\n",
    "#         # print(bases)\n",
    "#         print(\"Before:\", working_usage.iloc[indexes[0]:indexes[1]+6,:])\n",
    "#     #https://stackoverflow.com/questions/47136436/python-pandas-convert-value-counts-output-to-dataframe\n",
    "#     base_counts = working_usage.loc[indexes[0]:indexes[1]+6,:]['loc'].value_counts().rename_axis('loc').reset_index(name='counts')\n",
    "#     # https://stackoverflow.com/questions/37841525/correct-way-to-set-value-on-a-slice-in-pandas\n",
    "#     base_1 = base_counts.loc[base_counts['loc'] == bases[0], 'counts'].iloc[0]\n",
    "#     base_2 = base_counts.loc[base_counts['loc'] == bases[1], 'counts'].iloc[0]\n",
    "#     if base_2 > base_1:\n",
    "#         working_usage.loc[indexes[0]:indexes[1]+6,:].loc[working_usage.loc[indexes[0]:indexes[1]+6,:]['loc'] == bases[0], 'loc'] = bases[1]\n",
    "#         # replace base 1\n",
    "#     else:\n",
    "#         working_usage.loc[indexes[0]:indexes[1]+6,:].loc[working_usage.loc[indexes[0]:indexes[1]+6,:]['loc'] == bases[1],'loc'] = bases[0]\n",
    "#         # replace base 2\n",
    "#         # also repalce base 2 if equal since base 1 came first\n",
    "#     if count % 50 == 0:\n",
    "#         print(\"After:\", working_usage.iloc[indexes[0]:indexes[1]+6,:])\n",
    "#     count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         uid           timestamp   loc app_id  traffic\n",
       "4050241  979 2016-04-21 07:44:42  3306    763    0.014\n",
       "4050242  979 2016-04-21 07:44:42  9251      2    0.001\n",
       "4050243  979 2016-04-21 07:44:42  9251    763    0.002\n",
       "4050244  979 2016-04-21 07:44:43  3306      2    0.016\n",
       "4050245  979 2016-04-21 07:44:43  9251      2    0.033\n",
       "4050246  979 2016-04-21 07:44:44  9251      2    0.003\n",
       "4050247  979 2016-04-21 07:44:47  9251      2    0.002\n",
       "4050248  979 2016-04-21 07:44:48  3306      2    0.054\n",
       "4050249  979 2016-04-21 07:44:48  9251      2    0.086\n",
       "4050250  979 2016-04-21 07:44:49  3306      2    0.003\n",
       "4050251  979 2016-04-21 07:44:50  9251      2    0.001\n",
       "4050252  979 2016-04-21 07:44:52  3306      2    0.001\n",
       "4050253  979 2016-04-21 07:44:53  3306      2    0.130\n",
       "4050254  979 2016-04-21 07:44:54  9251      2    0.021\n",
       "4050255  979 2016-04-21 07:44:59  9251      2    0.004\n",
       "4050256  979 2016-04-21 07:45:01  3306      2    0.002\n",
       "4050257  979 2016-04-21 07:45:01  9251     77    0.185\n",
       "4050258  979 2016-04-21 07:45:05  9251      2    0.003"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uid</th>\n      <th>timestamp</th>\n      <th>loc</th>\n      <th>app_id</th>\n      <th>traffic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4050241</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:42</td>\n      <td>3306</td>\n      <td>763</td>\n      <td>0.014</td>\n    </tr>\n    <tr>\n      <th>4050242</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:42</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>4050243</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:42</td>\n      <td>9251</td>\n      <td>763</td>\n      <td>0.002</td>\n    </tr>\n    <tr>\n      <th>4050244</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:43</td>\n      <td>3306</td>\n      <td>2</td>\n      <td>0.016</td>\n    </tr>\n    <tr>\n      <th>4050245</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:43</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.033</td>\n    </tr>\n    <tr>\n      <th>4050246</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:44</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.003</td>\n    </tr>\n    <tr>\n      <th>4050247</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:47</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.002</td>\n    </tr>\n    <tr>\n      <th>4050248</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:48</td>\n      <td>3306</td>\n      <td>2</td>\n      <td>0.054</td>\n    </tr>\n    <tr>\n      <th>4050249</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:48</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.086</td>\n    </tr>\n    <tr>\n      <th>4050250</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:49</td>\n      <td>3306</td>\n      <td>2</td>\n      <td>0.003</td>\n    </tr>\n    <tr>\n      <th>4050251</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:50</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>4050252</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:52</td>\n      <td>3306</td>\n      <td>2</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>4050253</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:53</td>\n      <td>3306</td>\n      <td>2</td>\n      <td>0.130</td>\n    </tr>\n    <tr>\n      <th>4050254</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:54</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.021</td>\n    </tr>\n    <tr>\n      <th>4050255</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:59</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.004</td>\n    </tr>\n    <tr>\n      <th>4050256</th>\n      <td>979</td>\n      <td>2016-04-21 07:45:01</td>\n      <td>3306</td>\n      <td>2</td>\n      <td>0.002</td>\n    </tr>\n    <tr>\n      <th>4050257</th>\n      <td>979</td>\n      <td>2016-04-21 07:45:01</td>\n      <td>9251</td>\n      <td>77</td>\n      <td>0.185</td>\n    </tr>\n    <tr>\n      <th>4050258</th>\n      <td>979</td>\n      <td>2016-04-21 07:45:05</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.003</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 48.7 ms\n"
     ]
    }
   ],
   "source": [
    "usage.iloc[4050241:4050259]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         uid           timestamp   loc app_id  traffic\n",
       "4050241  979 2016-04-21 07:44:42  9251    763    0.014\n",
       "4050242  979 2016-04-21 07:44:42  9251      2    0.001\n",
       "4050243  979 2016-04-21 07:44:42  9251    763    0.002\n",
       "4050244  979 2016-04-21 07:44:43  9251      2    0.016\n",
       "4050245  979 2016-04-21 07:44:43  9251      2    0.033\n",
       "4050246  979 2016-04-21 07:44:44  9251      2    0.003\n",
       "4050247  979 2016-04-21 07:44:47  9251      2    0.002\n",
       "4050248  979 2016-04-21 07:44:48  9251      2    0.054\n",
       "4050249  979 2016-04-21 07:44:48  9251      2    0.086\n",
       "4050250  979 2016-04-21 07:44:49  9251      2    0.003\n",
       "4050251  979 2016-04-21 07:44:50  9251      2    0.001\n",
       "4050252  979 2016-04-21 07:44:52  9251      2    0.001\n",
       "4050253  979 2016-04-21 07:44:53  9251      2    0.130\n",
       "4050254  979 2016-04-21 07:44:54  9251      2    0.021\n",
       "4050255  979 2016-04-21 07:44:59  9251      2    0.004\n",
       "4050256  979 2016-04-21 07:45:01  9251      2    0.002\n",
       "4050257  979 2016-04-21 07:45:01  9251     77    0.185\n",
       "4050258  979 2016-04-21 07:45:05  9251      2    0.003\n",
       "4050259  979 2016-04-21 07:45:06  9251      2    0.001"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uid</th>\n      <th>timestamp</th>\n      <th>loc</th>\n      <th>app_id</th>\n      <th>traffic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4050241</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:42</td>\n      <td>9251</td>\n      <td>763</td>\n      <td>0.014</td>\n    </tr>\n    <tr>\n      <th>4050242</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:42</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>4050243</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:42</td>\n      <td>9251</td>\n      <td>763</td>\n      <td>0.002</td>\n    </tr>\n    <tr>\n      <th>4050244</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:43</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.016</td>\n    </tr>\n    <tr>\n      <th>4050245</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:43</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.033</td>\n    </tr>\n    <tr>\n      <th>4050246</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:44</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.003</td>\n    </tr>\n    <tr>\n      <th>4050247</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:47</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.002</td>\n    </tr>\n    <tr>\n      <th>4050248</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:48</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.054</td>\n    </tr>\n    <tr>\n      <th>4050249</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:48</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.086</td>\n    </tr>\n    <tr>\n      <th>4050250</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:49</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.003</td>\n    </tr>\n    <tr>\n      <th>4050251</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:50</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>4050252</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:52</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.001</td>\n    </tr>\n    <tr>\n      <th>4050253</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:53</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.130</td>\n    </tr>\n    <tr>\n      <th>4050254</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:54</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.021</td>\n    </tr>\n    <tr>\n      <th>4050255</th>\n      <td>979</td>\n      <td>2016-04-21 07:44:59</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.004</td>\n    </tr>\n    <tr>\n      <th>4050256</th>\n      <td>979</td>\n      <td>2016-04-21 07:45:01</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.002</td>\n    </tr>\n    <tr>\n      <th>4050257</th>\n      <td>979</td>\n      <td>2016-04-21 07:45:01</td>\n      <td>9251</td>\n      <td>77</td>\n      <td>0.185</td>\n    </tr>\n    <tr>\n      <th>4050258</th>\n      <td>979</td>\n      <td>2016-04-21 07:45:05</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.003</td>\n    </tr>\n    <tr>\n      <th>4050259</th>\n      <td>979</td>\n      <td>2016-04-21 07:45:06</td>\n      <td>9251</td>\n      <td>2</td>\n      <td>0.001</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 14.9 ms\n"
     ]
    }
   ],
   "source": [
    "working_usage.loc[4050241:4050259]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "working_usage.to_csv('rm_oscillated_data.txt', header=False, index=False, sep=',')"
   ]
  },
  {
   "source": [
    "# Handle the oscillatoin problem"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Hybrid Approach"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 351 µs\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 280 µs\n"
     ]
    }
   ],
   "source": [
    "# # Function to align the Origin and Destination (O/D)\n",
    "# def sort_movement_tuples(jump):\n",
    "#     return sorted(list(jump))\n",
    "\n",
    "# # Define a function that can output the frequency pair of a user's mobility pattern\n",
    "# def show_frequent_pair(uid=888, threshold=5, jump_occurence=10):\n",
    "\n",
    "#     user_uid = usage[usage['uid'] == str(uid)]\n",
    "\n",
    "#     # Set the thresold of time gap allowed between movement\n",
    "#     delta_threshold = timedelta(seconds=threshold)\n",
    "\n",
    "#     # Create the columns of the next time and lcoation\n",
    "#     user_uid['next_loc'] = user_uid['loc'].shift(-1).fillna(0).astype('int')\n",
    "#     user_uid['next_timestamp'] = user_uid['timestamp'].shift(-1)\n",
    "\n",
    "#     # Keep only movements\n",
    "#     user_uid = user_uid[user_uid['loc'] != user_uid['next_loc']]\n",
    "\n",
    "#     # Get the time gaps between movements\n",
    "#     user_uid['time_delta'] = user_uid['next_timestamp'] - user_uid['timestamp']\n",
    "\n",
    "#     # Get teleports and count the teleports\n",
    "#     # Should be able to identify the return trips\n",
    "#     teleports = user_uid[user_uid['time_delta'] < delta_threshold]\n",
    "#     teleports['tele'] = list(zip(teleports['loc'], teleports['next_loc']))\n",
    "#     # print(teleports['tele'].value_counts())\n",
    "\n",
    "#     # Group and count the same jump\n",
    "#     tele_counts = teleports['tele'].value_counts().reset_index()\n",
    "#     tele_counts.columns = ['jump', 'count']\n",
    "\n",
    "#     # Align the O/D of the jump to identify the pairs\n",
    "#     tele_counts['jump'] = tele_counts['jump'].apply(lambda i: sort_movement_tuples(i))\n",
    "\n",
    "#     # Get the jumps that has a reasonable number of occurence\n",
    "#     tele_counts = tele_counts[tele_counts['count'] > jump_occurence]\n",
    "#     tele_counts['jump'] = tuple(tele_counts['jump'])\n",
    "\n",
    "#     # print the record for reference\n",
    "#     frequent_pairs = tele_counts.groupby(['jump']).sum()\n",
    "#     # print(frequent_pairs)\n",
    "\n",
    "#     return frequent_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 548 µs\n"
     ]
    }
   ],
   "source": [
    "# # Show the frequent pairs of user 13, gap less than 3 seconds, and occurence higher than 5 times (both directions count)\n",
    "# freq_pair = show_frequent_pair(13,3,5)\n",
    "# # freq_pair.head(20)"
   ]
  },
  {
   "source": [
    "### Algorithm to approximate the baseID clusters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 339 µs\n"
     ]
    }
   ],
   "source": [
    "# # experiment = ['888','772','837']\n",
    "# experiment = [772]\n",
    "\n",
    "# base_cluster = []\n",
    "# skipped_jump = []\n",
    "\n",
    "# for user in experiment:\n",
    "#     freq_pair = show_frequent_pair(user,1,5)\n",
    "\n",
    "#     # Loop over the unique jump pattern\n",
    "#     for i in freq_pair.index:\n",
    "\n",
    "#         # Get the first and second baseID\n",
    "#         loc1 = i[0]\n",
    "#         loc2 = i[1]\n",
    "        \n",
    "#         # Start storing clusters\n",
    "#         if len(base_cluster) == 0:\n",
    "#             base_cluster.append(list(i)) # Add the first jump to the cluster\n",
    "\n",
    "#         # Start expanding or creating new clusters\n",
    "#         # Loop through each cluster\n",
    "#         for pos in range(len(base_cluster)):\n",
    "\n",
    "#             # if loc1 is in the cluster, we assume loc2 is a base station near the cluster. Vice versa\n",
    "#             # Then we add both baseID to the cluster. Either one of them is already in there anway\n",
    "#             if loc1 == base_cluster[pos][0]:\n",
    "                \n",
    "#                 # print(list(i))\n",
    "#                 # We add the new baseID to the existing cluster\n",
    "#                 base_cluster[pos] += list(i)\n",
    "#                 # base_cluster[pos]  = set(a)\n",
    "#                 new_cluster = 0 # Saying we do not need to create a new cluster\n",
    "#                 break\n",
    "\n",
    "#             # If one of the baseID is linked to the cluster, we flag up to create a new cluster\n",
    "#             else:\n",
    "#                 new_cluster = 1\n",
    "                \n",
    "#         # Save the jump later for the reason below\n",
    "#         if new_cluster == 1:\n",
    "#             base_cluster.append(list(i))\n",
    "#             new_cluster = 0\n",
    "\n",
    "# # Previous method\n",
    "# # Issue happens a the end of the loop. A jump with two new baseID created a new cluster. Then the next jump has one baseID connected to previous cluster. In that case, the new cluster created in the previous jump should be included in the bigger cluster. Therefore, we skipped the jumps and process it later in the cell below\n",
    "\n",
    "\n",
    "# # a = [sorted(set(list(x))) for x in base_cluster]\n",
    "# print(\"We approximated {} clusters.\".format(len(base_cluster)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 290 µs\n"
     ]
    }
   ],
   "source": [
    "# base_mapper = {}\n",
    "# for i in base_cluster:\n",
    "#     base_mapper[i[0]] = sorted(list(set(i[1:])))\n",
    "\n",
    "# # pprint(base_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 722 µs\n"
     ]
    }
   ],
   "source": [
    "# a = [set(list(x)) for x in base_cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 462 µs\n"
     ]
    }
   ],
   "source": [
    "# # We count the occurence of baseID from all cluster\n",
    "# # I assume the baseID will not repeat but it is not\n",
    "# count_occurence = {}\n",
    "# for combo in a:\n",
    "#     for place in combo:\n",
    "#         if place in count_occurence.keys():\n",
    "#             count_occurence[place] += 1\n",
    "#         else:\n",
    "#             count_occurence[place] = 1\n",
    "\n",
    "# # Show results in a dictionary\n",
    "# from collections import Counter\n",
    "# results = dict(Counter(list(count_occurence.values())))\n",
    "# print(results)\n",
    "# print()\n",
    "# print(\"Some baseID appear in more than one clusters\")\n",
    "# print(\"Have to investigate why\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 410 µs\n"
     ]
    }
   ],
   "source": [
    "# [[k, v] for k, v in sorted(count_occurence.items(), key=lambda item: item[1], reverse=True)][:10]"
   ]
  },
  {
   "source": [
    "## Speed Approach"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 1.68 ms\n"
     ]
    }
   ],
   "source": [
    "# def reduce_noise(uid=888, threshold=300, show_graph=False):\n",
    "\n",
    "#     user_uid = usage[usage['uid'] == str(uid)]\n",
    "#     user_uid.reset_index(inplace=True, drop=True)\n",
    "#     user_uid.drop(['app_id', 'traffic'], axis=1, inplace=True)\n",
    "\n",
    "#     # Set the thresold of time gap allowed between movement\n",
    "#     delta_threshold = timedelta(seconds=threshold)\n",
    "\n",
    "#     # Create the columns of the previous time and location\n",
    "#     user_uid['prev_loc'] = user_uid['loc'].shift(1).fillna(9999).astype('int')\n",
    "#     user_uid['prev_timestamp'] = user_uid['timestamp'].shift(1)\n",
    "#     user_uid['original'] = user_uid['loc']\n",
    "#     # Get the time gaps between movements\n",
    "#     user_uid['time_delta'] =  user_uid['timestamp'] - user_uid['prev_timestamp']\n",
    "\n",
    "\n",
    "#     origin_user_uid = user_uid.copy()\n",
    "\n",
    "#     before = user_uid['loc'].value_counts().shape[0]\n",
    "\n",
    "#     for i in user_uid[user_uid['time_delta'] < timedelta(seconds=threshold)].index:\n",
    "#         user_uid.iloc[i,2] = user_uid.iloc[i-1,2]\n",
    "\n",
    "#     after = user_uid['loc'].value_counts().shape[0]\n",
    "\n",
    "\n",
    "#     if show_graph == True:\n",
    "\n",
    "#         print(\"=\"*50)\n",
    "#         print(\"From {} to {} unique_base...\".format(before, after))\n",
    "#         print(\"=\"*50)\n",
    "#         print()\n",
    "        \n",
    "#         clean_user_uid = user_uid.copy()\n",
    "\n",
    "#         fig, axs = plt.subplots(2, figsize=(15,3))\n",
    "#         fig.suptitle('Uid: {}'.format(uid))\n",
    "\n",
    "#         # plt.figure(figsize=(15,3))\n",
    "#         axs[0].scatter(origin_user_uid['timestamp'],origin_user_uid['loc'], alpha=0.5)\n",
    "#         axs[1].scatter(clean_user_uid['timestamp'],clean_user_uid['loc'], alpha=0.5)\n",
    "#         plt.show()\n",
    "    \n",
    "#     return int(before - after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 1.3 ms\n"
     ]
    }
   ],
   "source": [
    "# sample_group = [10, 17, 888]\n",
    "# threshold = 1200 # seconds\n",
    "\n",
    "# reduce_result = []\n",
    "# for u in sample_group:\n",
    "#     reduce_result.append(reduce_noise(u, threshold, True)) # \n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 298 µs\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 278 µs\n"
     ]
    }
   ],
   "source": [
    "# usage.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}